{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)\n",
    "    self.wk = nn.Linear(input_dim, d_model)\n",
    "    self.wv = nn.Linear(input_dim, d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # 입력 x: (B, S, input_dim) - 배치 크기, 시퀀스 길이, 입력 차원\n",
    "    print(f\"입력 x shape: {x.shape}\")\n",
    "    \n",
    "    # Linear 변환을 통한 Query, Key, Value 생성\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)  # 각각 (B, S, d_model)\n",
    "    print(f\"q, k, v shape: {q.shape}, {k.shape}, {v.shape}\")\n",
    "    \n",
    "    B, S, D = q.shape[0], q.shape[1], self.d_model // self.n_heads\n",
    "    print(f\"B(배치 크기): {B}, S(시퀀스 길이): {S}, D(헤드별 차원): {D}\")\n",
    "\n",
    "    # 각 헤드별로 분할하기 위해 reshape 및 transpose\n",
    "    q = q.reshape((B, S, self.n_heads, D)).transpose(1, 2)  # (B, n_heads, S, D)\n",
    "    k = k.reshape((B, S, self.n_heads, D)).transpose(1, 2)  # (B, n_heads, S, D)\n",
    "    v = v.reshape((B, S, self.n_heads, D)).transpose(1, 2)  # (B, n_heads, S, D)\n",
    "    # q,k,v shape: torch.Size([2, 4, 5, 16]) \n",
    "    print(f\"분할 후 q shape: {q.shape}\")\n",
    "    print(f\"분할 후 k shape: {k.shape}\")\n",
    "    print(f\"분할 후 v shape: {v.shape}\")\n",
    "\n",
    "    # 어텐션 스코어 계산\n",
    "    score = torch.matmul(q, k.transpose(-1, -2))  # (B, n_heads, S, S)\n",
    "    print(f\"스코어 계산 후 shape: {score.shape}\")\n",
    "    \n",
    "    # 스케일링\n",
    "    score = score / sqrt(self.d_model)\n",
    "    print(f\"스케일링 후 score shape: {score.shape}\")\n",
    "\n",
    "    # 마스킹 적용 (필요한 경우)\n",
    "    if mask is not None:\n",
    "      print(f\"마스크 shape: {mask.shape}\")\n",
    "      # mask: (B, S, 1) -> mask[:, None]: (B, 1, S, 1)\n",
    "      \n",
    "\n",
    "      # 마스크 shape: torch.Size([2, 5, 1])\n",
    "      # 마스킹 후 score shape: torch.Size([2, 4, 5, 5])     \n",
    "\n",
    "      score = score + (mask[:, None] * -1e9)\n",
    "      print(f\"마스킹 후 score shape: {score.shape}\")\n",
    "\n",
    "    # 소프트맥스 적용\n",
    "    score = self.softmax(score)  # (B, n_heads, S, S)\n",
    "    print(f\"소프트맥스 후 score shape: {score.shape}\")\n",
    "    \n",
    "    # Value와 가중합 계산\n",
    "    result = torch.matmul(score, v)  # (B, n_heads, S, D)\n",
    "    print(f\"어텐션 결과 shape: {result.shape}\")\n",
    "\n",
    "    # 헤드 병합\n",
    "    result = result.transpose(1, 2).reshape((B, S, -1))  # (B, S, d_model)\n",
    "    print(f\"헤드 병합 후 shape: {result.shape}\")\n",
    "    \n",
    "    # 최종 출력층\n",
    "    result = self.dense(result)  # (B, S, d_model)\n",
    "    print(f\"최종 출력 shape: {result.shape}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 생성: torch.Size([4, 10, 32])\n",
      "마스크 생성: torch.Size([4, 10, 1])\n",
      "마스크 예시 (배치 0): tensor([False, False, False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "--- 모델 초기화 완료 ---\n",
      "입력 차원: 32, 모델 차원: 64, 헤드 수: 4\n",
      "\n",
      "--- 모델 실행 ---\n",
      "입력 x shape: torch.Size([4, 10, 32])\n",
      "q, k, v shape: torch.Size([4, 10, 64]), torch.Size([4, 10, 64]), torch.Size([4, 10, 64])\n",
      "B(배치 크기): 4, S(시퀀스 길이): 10, D(헤드별 차원): 16\n",
      "분할 후 q shape: torch.Size([4, 4, 10, 16])\n",
      "분할 후 k shape: torch.Size([4, 4, 10, 16])\n",
      "분할 후 v shape: torch.Size([4, 4, 10, 16])\n",
      "스코어 계산 후 shape: torch.Size([4, 4, 10, 10])\n",
      "스케일링 후 score shape: torch.Size([4, 4, 10, 10])\n",
      "마스크 shape: torch.Size([4, 10, 1])\n",
      "마스킹 후 score shape: torch.Size([4, 4, 10, 10])\n",
      "소프트맥스 후 score shape: torch.Size([4, 4, 10, 10])\n",
      "어텐션 결과 shape: torch.Size([4, 4, 10, 16])\n",
      "헤드 병합 후 shape: torch.Size([4, 10, 64])\n",
      "최종 출력 shape: torch.Size([4, 10, 64])\n",
      "\n",
      "--- 결과 요약 ---\n",
      "입력 형태: torch.Size([4, 10, 32])\n",
      "출력 형태: torch.Size([4, 10, 64])\n",
      "각 헤드의 차원: 16\n",
      "\n",
      "--- 결과 샘플 ---\n",
      "출력 첫 번째 토큰: tensor([-0.0995,  0.0652,  0.1418,  0.0284,  0.1384], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 테스트를 위한 MultiHeadAttention 클래스 (앞서 정의한 코드 사용)\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     ...\n",
    "\n",
    "# 테스트 파라미터 설정\n",
    "batch_size = 4\n",
    "seq_length = 10\n",
    "input_dim = 32\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "\n",
    "# 임의의 입력 텐서 생성\n",
    "x = torch.randn(batch_size, seq_length, input_dim)\n",
    "print(f\"입력 텐서 생성: {x.shape}\")\n",
    "\n",
    "# 마스크 생성 (일부 위치를 패딩으로 표시)\n",
    "# True는 패딩 토큰을 나타냄\n",
    "mask = torch.zeros(batch_size, seq_length, 1, dtype=torch.bool)\n",
    "for i in range(batch_size):\n",
    "    # 각 배치마다 다른 길이의 패딩 설정 (예시)\n",
    "    pad_length = torch.randint(0, seq_length//2, (1,)).item()\n",
    "    mask[i, -pad_length:] = True\n",
    "print(f\"마스크 생성: {mask.shape}\")\n",
    "print(f\"마스크 예시 (배치 0): {mask[0].squeeze()}\")\n",
    "\n",
    "# 모델 초기화\n",
    "attention = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "print(f\"\\n--- 모델 초기화 완료 ---\")\n",
    "print(f\"입력 차원: {input_dim}, 모델 차원: {d_model}, 헤드 수: {n_heads}\")\n",
    "\n",
    "# 모델 실행\n",
    "print(f\"\\n--- 모델 실행 ---\")\n",
    "output = attention(x, mask)\n",
    "\n",
    "print(f\"\\n--- 결과 요약 ---\")\n",
    "print(f\"입력 형태: {x.shape}\")\n",
    "print(f\"출력 형태: {output.shape}\")\n",
    "print(f\"각 헤드의 차원: {d_model//n_heads}\")\n",
    "\n",
    "# 첫 번째 배치의 첫 토큰 값 확인\n",
    "print(f\"\\n--- 결과 샘플 ---\")\n",
    "print(f\"출력 첫 번째 토큰: {output[0, 0, :5]}\")  # 첫 5개 요소만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 다양한 입력 크기 테스트 ===\n",
      "\n",
      "테스트 케이스 1: 배치=2, 시퀀스=5, 차원=32\n",
      "입력 x shape: torch.Size([2, 5, 32])\n",
      "q, k, v shape: torch.Size([2, 5, 64]), torch.Size([2, 5, 64]), torch.Size([2, 5, 64])\n",
      "B(배치 크기): 2, S(시퀀스 길이): 5, D(헤드별 차원): 16\n",
      "분할 후 q shape: torch.Size([2, 4, 5, 16])\n",
      "분할 후 k shape: torch.Size([2, 4, 5, 16])\n",
      "분할 후 v shape: torch.Size([2, 4, 5, 16])\n",
      "스코어 계산 후 shape: torch.Size([2, 4, 5, 5])\n",
      "스케일링 후 score shape: torch.Size([2, 4, 5, 5])\n",
      "마스크 shape: torch.Size([2, 5, 1])\n",
      "마스킹 후 score shape: torch.Size([2, 4, 5, 5])\n",
      "소프트맥스 후 score shape: torch.Size([2, 4, 5, 5])\n",
      "어텐션 결과 shape: torch.Size([2, 4, 5, 16])\n",
      "헤드 병합 후 shape: torch.Size([2, 5, 64])\n",
      "최종 출력 shape: torch.Size([2, 5, 64])\n",
      "  입력: torch.Size([2, 5, 32]) -> 출력: torch.Size([2, 5, 64])\n",
      "\n",
      "테스트 케이스 2: 배치=8, 시퀀스=20, 차원=64\n",
      "입력 x shape: torch.Size([8, 20, 64])\n",
      "q, k, v shape: torch.Size([8, 20, 64]), torch.Size([8, 20, 64]), torch.Size([8, 20, 64])\n",
      "B(배치 크기): 8, S(시퀀스 길이): 20, D(헤드별 차원): 16\n",
      "분할 후 q shape: torch.Size([8, 4, 20, 16])\n",
      "분할 후 k shape: torch.Size([8, 4, 20, 16])\n",
      "분할 후 v shape: torch.Size([8, 4, 20, 16])\n",
      "스코어 계산 후 shape: torch.Size([8, 4, 20, 20])\n",
      "스케일링 후 score shape: torch.Size([8, 4, 20, 20])\n",
      "마스크 shape: torch.Size([8, 20, 1])\n",
      "마스킹 후 score shape: torch.Size([8, 4, 20, 20])\n",
      "소프트맥스 후 score shape: torch.Size([8, 4, 20, 20])\n",
      "어텐션 결과 shape: torch.Size([8, 4, 20, 16])\n",
      "헤드 병합 후 shape: torch.Size([8, 20, 64])\n",
      "최종 출력 shape: torch.Size([8, 20, 64])\n",
      "  입력: torch.Size([8, 20, 64]) -> 출력: torch.Size([8, 20, 64])\n",
      "\n",
      "테스트 케이스 3: 배치=1, 시퀀스=100, 차원=32\n",
      "입력 x shape: torch.Size([1, 100, 32])\n",
      "q, k, v shape: torch.Size([1, 100, 64]), torch.Size([1, 100, 64]), torch.Size([1, 100, 64])\n",
      "B(배치 크기): 1, S(시퀀스 길이): 100, D(헤드별 차원): 16\n",
      "분할 후 q shape: torch.Size([1, 4, 100, 16])\n",
      "분할 후 k shape: torch.Size([1, 4, 100, 16])\n",
      "분할 후 v shape: torch.Size([1, 4, 100, 16])\n",
      "스코어 계산 후 shape: torch.Size([1, 4, 100, 100])\n",
      "스케일링 후 score shape: torch.Size([1, 4, 100, 100])\n",
      "마스크 shape: torch.Size([1, 100, 1])\n",
      "마스킹 후 score shape: torch.Size([1, 4, 100, 100])\n",
      "소프트맥스 후 score shape: torch.Size([1, 4, 100, 100])\n",
      "어텐션 결과 shape: torch.Size([1, 4, 100, 16])\n",
      "헤드 병합 후 shape: torch.Size([1, 100, 64])\n",
      "최종 출력 shape: torch.Size([1, 100, 64])\n",
      "  입력: torch.Size([1, 100, 32]) -> 출력: torch.Size([1, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "# 다양한 입력 크기로 테스트\n",
    "test_cases = [\n",
    "    {\"batch\": 2, \"seq\": 5, \"dim\": 32},\n",
    "    {\"batch\": 8, \"seq\": 20, \"dim\": 64},\n",
    "    {\"batch\": 1, \"seq\": 100, \"dim\": 32}\n",
    "]\n",
    "\n",
    "print(\"\\n=== 다양한 입력 크기 테스트 ===\")\n",
    "for i, case in enumerate(test_cases):\n",
    "    b, s, d = case[\"batch\"], case[\"seq\"], case[\"dim\"]\n",
    "    print(f\"\\n테스트 케이스 {i+1}: 배치={b}, 시퀀스={s}, 차원={d}\")\n",
    "    \n",
    "    # 입력 값\n",
    "    \n",
    "    x_test = torch.randn(b, s, d)\n",
    "    mask_test = torch.zeros(b, s, 1, dtype=torch.bool)\n",
    "    \n",
    "    # 마스크 일부 설정\n",
    "    for j in range(b):\n",
    "        pad_len = s // 3  # 시퀀스의 1/3을 패딩으로 설정\n",
    "        mask_test[j, -pad_len:] = True\n",
    "    \n",
    "    # 각 테스트 케이스마다 새 모델 생성\n",
    "    test_attention = MultiHeadAttention(d, d_model, n_heads)\n",
    "    output_test = test_attention(x_test, mask_test)\n",
    "    \n",
    "    print(f\"  입력: {x_test.shape} -> 출력: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<think>\n",
    "질문은 마스크 테스트 값의 의미에 대해 묻고 있습니다. 코드에서 마스크를 어떻게 생성하고 사용하는지 분석해 봅시다.\n",
    "\n",
    "코드에서 마스크 생성 부분:\n",
    "```python\n",
    "# 마스크 생성 (일부 위치를 패딩으로 표시)\n",
    "# True는 패딩 토큰을 나타냄\n",
    "mask = torch.zeros(batch_size, seq_length, 1, dtype=torch.bool)\n",
    "for i in range(batch_size):\n",
    "    # 각 배치마다 다른 길이의 패딩 설정 (예시)\n",
    "    pad_length = torch.randint(0, seq_length//2, (1,)).item()\n",
    "    mask[i, -pad_length:] = True\n",
    "```\n",
    "\n",
    "여기서 알 수 있는 점:\n",
    "1. `mask`는 (batch_size, seq_length, 1) 형태의 boolean 텐서\n",
    "2. 기본적으로 모든 값이 False로 초기화\n",
    "3. 각 배치마다 다르게 시퀀스의 뒷부분 일부를 True로 설정\n",
    "4. True는 패딩 토큰을 나타냄\n",
    "\n",
    "그리고 MultiHeadAttention 클래스의 forward 메서드에서 마스크를 사용하는 부분:\n",
    "```python\n",
    "if mask is not None:\n",
    "  score = score + (mask[:, None] * -1e9)\n",
    "```\n",
    "\n",
    "이것은 패딩 토큰(True)에 매우 작은 음수 값을 더해 소프트맥스 후에 해당 위치의 가중치가 0에 가까워지게 만듭니다.\n",
    "\n",
    "따라서 마스크 테스트 값의 의미:\n",
    "1. False: 실제 토큰 (어텐션 계산에 참여)\n",
    "2. True: 패딩 토큰 (어텐션 계산에서 제외)\n",
    "</think>\n",
    "\n",
    "# 트랜스포머에서 마스크의 의미\n",
    "\n",
    "멀티헤드 어텐션에서 사용된 마스크 테스트 값은 **패딩 토큰을 식별**하는 용도로 사용됩니다.\n",
    "\n",
    "## 마스크 값의 의미\n",
    "\n",
    "```python\n",
    "# 테스트 코드에서 마스크 생성 부분\n",
    "mask = torch.zeros(batch_size, seq_length, 1, dtype=torch.bool)\n",
    "for i in range(batch_size):\n",
    "    pad_length = torch.randint(0, seq_length//2, (1,)).item()\n",
    "    mask[i, -pad_length:] = True\n",
    "```\n",
    "\n",
    "여기서:\n",
    "- **False (0)**: 실제 의미 있는 토큰\n",
    "- **True (1)**: 패딩 토큰 (무시해야 할 토큰)\n",
    "\n",
    "## 마스크 사용 방법\n",
    "\n",
    "```python\n",
    "if mask is not None:\n",
    "  score = score + (mask[:, None] * -1e9)\n",
    "```\n",
    "\n",
    "이 코드의 효과:\n",
    "1. **패딩 토큰 위치**(True 값)에 매우 큰 음수 값(-1e9)을 더함\n",
    "2. 소프트맥스 함수 적용 시 이 위치들은 0에 가까운 확률을 갖게 됨\n",
    "3. 결과적으로 패딩 토큰은 어텐션 계산에 기여하지 않음\n",
    "\n",
    "## 실제 응용 예시\n",
    "\n",
    "1. **가변 길이 시퀀스 처리**:\n",
    "   - 배치 처리를 위해 짧은 문장들은 패딩으로 채움\n",
    "   - 패딩 토큰은 의미가 없으므로 마스크로 처리\n",
    "\n",
    "2. **자기 회귀적(auto-regressive) 생성**:\n",
    "   - GPT와 같은 모델에서는 미래 토큰을 보지 못하게 함\n",
    "   - 이 경우 다른 형태의 마스크(Look-ahead 마스크)를 사용\n",
    "\n",
    "3. **BERT의 양방향 어텐션**:\n",
    "   - 패딩 마스크만 사용하여 양방향 문맥을 모두 고려\n",
    "\n",
    "마스크는 트랜스포머 모델이 다양한 길이의 시퀀스를 효율적으로 처리하고, 특정 토큰을 무시할 수 있게 해주는 핵심 기술입니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
