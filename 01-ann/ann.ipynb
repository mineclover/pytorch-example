{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 추가\n",
    "import os  # 추가\n",
    "import zipfile  # 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 다운로드 및 압축 해제 완료!\n"
     ]
    }
   ],
   "source": [
    "# # Google Drive에서 데이터 다운로드 ( 안씀 )\n",
    "output_path = \"data/\"\n",
    "# if not os.path.exists(output_path):\n",
    "#     gdown.download(\"https://drive.google.com/uc?id=1Pocm6FWifjOCrWz8wnMkHcrh4MJI3HVw\", output_path, quiet=False)\n",
    "\n",
    "\n",
    "# 압축 해제\n",
    "extract_path = \"data/MNIST/raw\"\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(output_path+\"MNIST.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"파일 다운로드 및 압축 해제 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST 데이터 다운로드 및 로드 완료!\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000022265630D50>\n"
     ]
    }
   ],
   "source": [
    "# 데이터 변환\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋 직접 다운로드 다운로드 용 스크립트임으로 셔플하지 않음?\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"MNIST 데이터 다운로드 및 로드 완료!\")\n",
    "print(testset)\n",
    "print(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGiNJREFUeJzt3X9o1Pcdx/HX1R9XdZcrQZO71JhlRdtNnaVq1WD90dXMQKX+KFjLRmRD2vmDif3BrAzTQY3YKUXSOldGpltt/WPWuinVDE10ZIo6XUWLWIwznQnBTO9i1EjMZ3+IR89Y9Xve+b5Lng/4grn7vr2P337r028u+cbnnHMCAMDAQ9YLAAB0X0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY6Wm9gFt1dHTo3LlzCgQC8vl81ssBAHjknFNLS4vy8vL00EN3vtZJuwidO3dO+fn51ssAANyn+vp6DRw48I77pN2n4wKBgPUSAABJcC9/n6csQh988IEKCwv18MMPa+TIkdq3b989zfEpOADoGu7l7/OURGjz5s1avHixli1bpiNHjuiZZ55RSUmJzp49m4qXAwBkKF8q7qI9ZswYPfXUU1q3bl3sse9///uaPn26ysvL7zgbjUYVDAaTvSQAwAMWiUSUlZV1x32SfiV07do1HT58WMXFxXGPFxcXq7a2ttP+bW1tikajcRsAoHtIeoTOnz+v69evKzc3N+7x3NxcNTY2dtq/vLxcwWAwtvGVcQDQfaTsCxNufUPKOXfbN6mWLl2qSCQS2+rr61O1JABAmkn69wn1799fPXr06HTV09TU1OnqSJL8fr/8fn+ylwEAyABJvxLq3bu3Ro4cqaqqqrjHq6qqVFRUlOyXAwBksJTcMWHJkiX66U9/qlGjRmncuHH6/e9/r7Nnz+rVV19NxcsBADJUSiI0e/ZsNTc36ze/+Y0aGho0bNgw7dixQwUFBal4OQBAhkrJ9wndD75PCAC6BpPvEwIA4F4RIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnpaLwBIJz169PA8EwwGU7CS5Fi4cGFCc3379vU88/jjj3ueWbBggeeZ3/72t55n5syZ43lGkq5evep5ZuXKlZ5n3n77bc8zXQVXQgAAM0QIAGAm6REqKyuTz+eL20KhULJfBgDQBaTkPaGhQ4fq73//e+zjRD7PDgDo+lISoZ49e3L1AwC4q5S8J3Tq1Cnl5eWpsLBQL730kk6fPv2t+7a1tSkajcZtAIDuIekRGjNmjDZu3KidO3fqww8/VGNjo4qKitTc3Hzb/cvLyxUMBmNbfn5+spcEAEhTSY9QSUmJZs2apeHDh+u5557T9u3bJUkbNmy47f5Lly5VJBKJbfX19cleEgAgTaX8m1X79eun4cOH69SpU7d93u/3y+/3p3oZAIA0lPLvE2pra9OXX36pcDic6pcCAGSYpEfo9ddfV01Njerq6nTgwAG9+OKLikajKi0tTfZLAQAyXNI/Hff1119rzpw5On/+vAYMGKCxY8dq//79KigoSPZLAQAyXNIj9MknnyT7t0SaGjRokOeZ3r17e54pKiryPDN+/HjPM5L0yCOPeJ6ZNWtWQq/V1Xz99deeZ9auXet5ZsaMGZ5nWlpaPM9I0r///W/PMzU1NQm9VnfFveMAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADM+55yzXsQ3RaNRBYNB62V0K08++WRCc7t37/Y8w3/bzNDR0eF55mc/+5nnmUuXLnmeSURDQ0NCcxcuXPA8c/LkyYReqyuKRCLKysq64z5cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMBMT+sFwN7Zs2cTmmtubvY8w120bzhw4IDnmYsXL3qemTx5sucZSbp27ZrnmT/96U8JvRa6N66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAU+t///pfQ3BtvvOF55vnnn/c8c+TIEc8za9eu9TyTqKNHj3qemTJliueZ1tZWzzNDhw71PCNJv/zlLxOaA7ziSggAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMONzzjnrRXxTNBpVMBi0XgZSJCsry/NMS0uL55n169d7npGkn//8555nfvKTn3ie+fjjjz3PAJkmEonc9f95roQAAGaIEADAjOcI7d27V9OmTVNeXp58Pp+2bt0a97xzTmVlZcrLy1OfPn00adIkHT9+PFnrBQB0IZ4j1NraqhEjRqiiouK2z69atUpr1qxRRUWFDh48qFAopClTpiT0eX0AQNfm+SerlpSUqKSk5LbPOef03nvvadmyZZo5c6YkacOGDcrNzdWmTZv0yiuv3N9qAQBdSlLfE6qrq1NjY6OKi4tjj/n9fk2cOFG1tbW3nWlra1M0Go3bAADdQ1Ij1NjYKEnKzc2Nezw3Nzf23K3Ky8sVDAZjW35+fjKXBABIYyn56jifzxf3sXOu02M3LV26VJFIJLbV19enYkkAgDTk+T2hOwmFQpJuXBGFw+HY401NTZ2ujm7y+/3y+/3JXAYAIEMk9UqosLBQoVBIVVVVsceuXbummpoaFRUVJfOlAABdgOcroUuXLumrr76KfVxXV6ejR48qOztbgwYN0uLFi7VixQoNHjxYgwcP1ooVK9S3b1+9/PLLSV04ACDzeY7QoUOHNHny5NjHS5YskSSVlpbqj3/8o958801duXJF8+fP14ULFzRmzBjt2rVLgUAgeasGAHQJ3MAUXdK7776b0NzNf1R5UVNT43nmueee8zzT0dHheQawxA1MAQBpjQgBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGa4iza6pH79+iU099e//tXzzMSJEz3PlJSUeJ7ZtWuX5xnAEnfRBgCkNSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADDcwBb7hscce8zzzr3/9y/PMxYsXPc/s2bPH88yhQ4c8z0jS+++/73kmzf4qQRrgBqYAgLRGhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhBqbAfZoxY4bnmcrKSs8zgUDA80yi3nrrLc8zGzdu9DzT0NDgeQaZgxuYAgDSGhECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghhuYAgaGDRvmeWbNmjWeZ370ox95nknU+vXrPc+88847nmf++9//ep6BDW5gCgBIa0QIAGDGc4T27t2radOmKS8vTz6fT1u3bo17fu7cufL5fHHb2LFjk7VeAEAX4jlCra2tGjFihCoqKr51n6lTp6qhoSG27dix474WCQDomnp6HSgpKVFJSckd9/H7/QqFQgkvCgDQPaTkPaHq6mrl5ORoyJAhmjdvnpqamr5137a2NkWj0bgNANA9JD1CJSUl+uijj7R7926tXr1aBw8e1LPPPqu2trbb7l9eXq5gMBjb8vPzk70kAECa8vzpuLuZPXt27NfDhg3TqFGjVFBQoO3bt2vmzJmd9l+6dKmWLFkS+zgajRIiAOgmkh6hW4XDYRUUFOjUqVO3fd7v98vv96d6GQCANJTy7xNqbm5WfX29wuFwql8KAJBhPF8JXbp0SV999VXs47q6Oh09elTZ2dnKzs5WWVmZZs2apXA4rDNnzuitt95S//79NWPGjKQuHACQ+TxH6NChQ5o8eXLs45vv55SWlmrdunU6duyYNm7cqIsXLyocDmvy5MnavHmzAoFA8lYNAOgSuIEpkCEeeeQRzzPTpk1L6LUqKys9z/h8Ps8zu3fv9jwzZcoUzzOwwQ1MAQBpjQgBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGa4izaATtra2jzP9Ozp/Qc1t7e3e5758Y9/7Hmmurra8wzuH3fRBgCkNSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADAjPc7DgK4bz/84Q89z7z44oueZ0aPHu15RkrsZqSJOHHihOeZvXv3pmAlsMKVEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghhuYAt/w+OOPe55ZuHCh55mZM2d6ngmFQp5nHqTr1697nmloaPA809HR4XkG6YsrIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADDcwRdpL5Madc+bMSei1ErkZ6Xe/+92EXiudHTp0yPPMO++843lm27ZtnmfQtXAlBAAwQ4QAAGY8Rai8vFyjR49WIBBQTk6Opk+frpMnT8bt45xTWVmZ8vLy1KdPH02aNEnHjx9P6qIBAF2DpwjV1NRowYIF2r9/v6qqqtTe3q7i4mK1trbG9lm1apXWrFmjiooKHTx4UKFQSFOmTFFLS0vSFw8AyGyevjDh888/j/u4srJSOTk5Onz4sCZMmCDnnN577z0tW7Ys9pMjN2zYoNzcXG3atEmvvPJK8lYOAMh49/WeUCQSkSRlZ2dLkurq6tTY2Kji4uLYPn6/XxMnTlRtbe1tf4+2tjZFo9G4DQDQPSQcIeeclixZovHjx2vYsGGSpMbGRklSbm5u3L65ubmx525VXl6uYDAY2/Lz8xNdEgAgwyQcoYULF+qLL77Qxx9/3Ok5n88X97FzrtNjNy1dulSRSCS21dfXJ7okAECGSeibVRctWqRt27Zp7969GjhwYOzxm99U2NjYqHA4HHu8qamp09XRTX6/X36/P5FlAAAynKcrIeecFi5cqC1btmj37t0qLCyMe76wsFChUEhVVVWxx65du6aamhoVFRUlZ8UAgC7D05XQggULtGnTJn322WcKBAKx93mCwaD69Okjn8+nxYsXa8WKFRo8eLAGDx6sFStWqG/fvnr55ZdT8gcAAGQuTxFat26dJGnSpElxj1dWVmru3LmSpDfffFNXrlzR/PnzdeHCBY0ZM0a7du1SIBBIyoIBAF2HzznnrBfxTdFoVMFg0HoZuAff9j7fnfzgBz/wPFNRUeF55oknnvA8k+4OHDjgeebdd99N6LU+++wzzzMdHR0JvRa6rkgkoqysrDvuw73jAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYCahn6yK9JWdne15Zv369Qm91pNPPul55nvf+15Cr5XOamtrPc+sXr3a88zOnTs9z1y5csXzDPAgcSUEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhBqYPyJgxYzzPvPHGG55nnn76ac8zjz76qOeZdHf58uWE5tauXet5ZsWKFZ5nWltbPc8AXRFXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5g+oDMmDHjgcw8SCdOnPA887e//c3zTHt7u+eZ1atXe56RpIsXLyY0ByAxXAkBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGZ8zjlnvYhvikajCgaD1ssAANynSCSirKysO+7DlRAAwAwRAgCY8RSh8vJyjR49WoFAQDk5OZo+fbpOnjwZt8/cuXPl8/nitrFjxyZ10QCArsFThGpqarRgwQLt379fVVVVam9vV3FxsVpbW+P2mzp1qhoaGmLbjh07krpoAEDX4Oknq37++edxH1dWVionJ0eHDx/WhAkTYo/7/X6FQqHkrBAA0GXd13tCkUhEkpSdnR33eHV1tXJycjRkyBDNmzdPTU1N3/p7tLW1KRqNxm0AgO4h4S/Rds7phRde0IULF7Rv377Y45s3b9Z3vvMdFRQUqK6uTr/+9a/V3t6uw4cPy+/3d/p9ysrK9Pbbbyf+JwAApKV7+RJtuQTNnz/fFRQUuPr6+jvud+7cOderVy/3l7/85bbPX7161UUikdhWX1/vJLGxsbGxZfgWiUTu2hJP7wndtGjRIm3btk179+7VwIED77hvOBxWQUGBTp06ddvn/X7/ba+QAABdn6cIOee0aNEiffrpp6qurlZhYeFdZ5qbm1VfX69wOJzwIgEAXZOnL0xYsGCB/vznP2vTpk0KBAJqbGxUY2Ojrly5Ikm6dOmSXn/9df3zn//UmTNnVF1drWnTpql///6aMWNGSv4AAIAM5uV9IH3L5/0qKyudc85dvnzZFRcXuwEDBrhevXq5QYMGudLSUnf27Nl7fo1IJGL+eUw2NjY2tvvf7uU9IW5gCgBICW5gCgBIa0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM2kXIeec9RIAAElwL3+fp12EWlparJcAAEiCe/n73OfS7NKjo6ND586dUyAQkM/ni3suGo0qPz9f9fX1ysrKMlqhPY7DDRyHGzgON3AcbkiH4+CcU0tLi/Ly8vTQQ3e+1un5gNZ0zx566CENHDjwjvtkZWV165PsJo7DDRyHGzgON3AcbrA+DsFg8J72S7tPxwEAug8iBAAwk1ER8vv9Wr58ufx+v/VSTHEcbuA43MBxuIHjcEOmHYe0+8IEAED3kVFXQgCAroUIAQDMECEAgBkiBAAwk1ER+uCDD1RYWKiHH35YI0eO1L59+6yX9ECVlZXJ5/PFbaFQyHpZKbd3715NmzZNeXl58vl82rp1a9zzzjmVlZUpLy9Pffr00aRJk3T8+HGbxabQ3Y7D3LlzO50fY8eOtVlsipSXl2v06NEKBALKycnR9OnTdfLkybh9usP5cC/HIVPOh4yJ0ObNm7V48WItW7ZMR44c0TPPPKOSkhKdPXvWemkP1NChQ9XQ0BDbjh07Zr2klGttbdWIESNUUVFx2+dXrVqlNWvWqKKiQgcPHlQoFNKUKVO63H0I73YcJGnq1Klx58eOHTse4ApTr6amRgsWLND+/ftVVVWl9vZ2FRcXq7W1NbZPdzgf7uU4SBlyPrgM8fTTT7tXX3017rEnnnjC/epXvzJa0YO3fPlyN2LECOtlmJLkPv3009jHHR0dLhQKuZUrV8Yeu3r1qgsGg+53v/udwQofjFuPg3POlZaWuhdeeMFkPVaampqcJFdTU+Oc677nw63HwbnMOR8y4kro2rVrOnz4sIqLi+MeLy4uVm1trdGqbJw6dUp5eXkqLCzUSy+9pNOnT1svyVRdXZ0aGxvjzg2/36+JEyd2u3NDkqqrq5WTk6MhQ4Zo3rx5ampqsl5SSkUiEUlSdna2pO57Ptx6HG7KhPMhIyJ0/vx5Xb9+Xbm5uXGP5+bmqrGx0WhVD96YMWO0ceNG7dy5Ux9++KEaGxtVVFSk5uZm66WZufnfv7ufG5JUUlKijz76SLt379bq1at18OBBPfvss2pra7NeWko457RkyRKNHz9ew4YNk9Q9z4fbHQcpc86HtLuL9p3c+qMdnHOdHuvKSkpKYr8ePny4xo0bp8cee0wbNmzQkiVLDFdmr7ufG5I0e/bs2K+HDRumUaNGqaCgQNu3b9fMmTMNV5YaCxcu1BdffKF//OMfnZ7rTufDtx2HTDkfMuJKqH///urRo0enf8k0NTV1+hdPd9KvXz8NHz5cp06dsl6KmZtfHci50Vk4HFZBQUGXPD8WLVqkbdu2ac+ePXE/+qW7nQ/fdhxuJ13Ph4yIUO/evTVy5EhVVVXFPV5VVaWioiKjVdlra2vTl19+qXA4bL0UM4WFhQqFQnHnxrVr11RTU9Otzw1Jam5uVn19fZc6P5xzWrhwobZs2aLdu3ersLAw7vnucj7c7TjcTtqeD4ZfFOHJJ5984nr16uX+8Ic/uBMnTrjFixe7fv36uTNnzlgv7YF57bXXXHV1tTt9+rTbv3+/e/75510gEOjyx6ClpcUdOXLEHTlyxElya9ascUeOHHH/+c9/nHPOrVy50gWDQbdlyxZ37NgxN2fOHBcOh100GjVeeXLd6Ti0tLS41157zdXW1rq6ujq3Z88eN27cOPfoo492qePwi1/8wgWDQVddXe0aGhpi2+XLl2P7dIfz4W7HIZPOh4yJkHPOvf/++66goMD17t3bPfXUU3FfjtgdzJ4924XDYderVy+Xl5fnZs6c6Y4fP269rJTbs2ePk9RpKy0tdc7d+LLc5cuXu1Ao5Px+v5swYYI7duyY7aJT4E7H4fLly664uNgNGDDA9erVyw0aNMiVlpa6s2fPWi87qW7355fkKisrY/t0h/Phbschk84HfpQDAMBMRrwnBADomogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM/8HVW8oTZjRdKUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "image, label = trainset[0]  # 첫 번째 이미지와 레이블 가져오기\n",
    "print(image.size())  # torch.Size([1, 28, 28]) 출력\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 이미지의 크기가 28 * 28인 것은 다음과 같은 방법으로 알 수 있습니다:\n",
    "\n",
    "1. **공식 문서와 논문**: MNIST 데이터셋은 컴퓨터 비전 분야에서 매우 잘 알려진 표준 데이터셋으로, 공식 문서와 관련 논문에 모든 이미지가 28×28 픽셀 크기로 정규화되어 있다고 명시되어 있습니다.\n",
    "\n",
    "2. **코드로 확인**: PyTorch나 TensorFlow 같은 라이브러리를 사용할 때 MNIST 데이터셋의 이미지 크기를 직접 확인할 수 있습니다:\n",
    "\n",
    "   ```python\n",
    "   import torchvision\n",
    "   trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "   image, label = trainset[0]  # 첫 번째 이미지와 레이블 가져오기\n",
    "   print(image.size())  # torch.Size([1, 28, 28]) 출력\n",
    "   ```\n",
    "\n",
    "3. **데이터셋 살펴보기**: 실제로 이미지를 시각화하고 특성을 살펴볼 수 있습니다:\n",
    "\n",
    "   ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "   plt.imshow(image.squeeze(), cmap='gray')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "4. **라이브러리 문서**: PyTorch나 TensorFlow의 문서에서 MNIST 데이터셋에 대한 설명을 확인하면 이미지 크기가 28×28임을 확인할 수 있습니다.\n",
    "\n",
    "5. **코드에서의 단서**: 질문에서 보여주신 코드에서 입력층의 크기가 28 * 28 = 784로 설정된 것을 통해 각 이미지가 784개의 픽셀(28×28)로 구성되어 있음을 유추할 수 있습니다.\n",
    "\n",
    "MNIST 데이터셋은 손글씨 숫자 인식을 위한 벤치마크 데이터셋으로, 모든 이미지가 일관된 28×28 픽셀 크기로 정규화되어 있어 딥러닝 모델의 학습에 편리하게 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "데이터에 표준과 평균을 정의하는게 좋다\n",
    "정의된 데이터는 여러개 있고 정의된 객체들의 데이터 범위가 서로 달라서 표준화 해줘야 함\n",
    "이 기능이 데이터 노멀라이즈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch shape: torch.Size([64, 1, 28, 28])\n",
      "Test labels: tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 변환\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # MNIST는 흑백 이미지이므로 변환 필요\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ImageFolder를 사용하여 데이터 로드\n",
    "data_dir = \"data/MNIST/raw\"\n",
    "\n",
    "# 다운로드한 MNIST 데이터 로드\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 데이터 확인\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f\"Test batch shape: {images.shape}\")\n",
    "print(f\"Test labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 ANN 모델 정의\n",
    "\n",
    "- `torch.nn.Module`: 모든 신경망 모듈의 기본 클래스입니다.  사용자 정의 신경망은 이 클래스를 상속받아야 합니다.\n",
    "    - 이걸로 기본적인 신경망 정의를 가져옴\n",
    "    \n",
    "- `nn.Linear`: 선형 변환을 적용하는 완전 연결(fully connected) 레이어를 정의합니다.\n",
    "    - nn.Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다.\n",
    "- `torch.relu`: ReLU 활성화 함수를 적용합니다.\n",
    "- `view`: 텐서의 크기를 변경합니다.\n",
    "    - x.view(-1, 28 * 28)은 입력 이미지를 1차원 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 하나의 이미지에 대해 64개의 배치로 들어오기 떄문에 아무튼 2차원으로 넘어오긴 하는데\n",
    "그게 왜 28 * 28 인지 모르겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fully connected 레이어는 모든 입력과 출력이 연결되어 있는 레이어 : fc\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        # 입력과 출력에 대한 정의를 한다\n",
    "        # 왜 28 * 28 인지 모르겠음\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # 입력층에서 은닉층으로 \n",
    "        # 입력층에서 128개를 보냄을 정의함\n",
    "        self.fc2 = nn.Linear(128, 64)       # 은닉층에서 은닉층으로\n",
    "        # 은닉층에서 64개를 보냄을 정의함\n",
    "        self.fc3 = nn.Linear(64, 10)        # 은닉층에서 출력층으로\n",
    "        # 출력층에서 10개를 보냄을 정의함\n",
    "        # 최종적으로 10개의 퍼셉트론을 받았음\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 28 * 28 의 이미지를 1차원 벡터로 변환\n",
    "        x = x.view(-1, 28 * 28)  # 입력 이미지를 1차원 벡터로 변환\n",
    "        # 1차원 벡터를 통해 128개를 받음\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # 128개를 통해 64개를 받음\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # 64개를 통해 10개를 보냄\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://wikidocs.net/60324\n",
    "forward 에서 레이어를 하나 씩 인식?\n",
    "x.view 를 통해 이미지를 1차원 벡터로 변환함\n",
    "relu 는 활성화 함수\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 1.343\n",
      "[Epoch 1, Batch 200] loss: 0.483\n",
      "[Epoch 1, Batch 300] loss: 0.393\n",
      "[Epoch 1, Batch 400] loss: 0.318\n",
      "[Epoch 1, Batch 500] loss: 0.319\n",
      "[Epoch 1, Batch 600] loss: 0.299\n",
      "[Epoch 1, Batch 700] loss: 0.278\n",
      "[Epoch 1, Batch 800] loss: 0.242\n",
      "[Epoch 1, Batch 900] loss: 0.245\n",
      "[Epoch 2, Batch 100] loss: 0.229\n",
      "[Epoch 2, Batch 200] loss: 0.201\n",
      "[Epoch 2, Batch 300] loss: 0.208\n",
      "[Epoch 2, Batch 400] loss: 0.195\n",
      "[Epoch 2, Batch 500] loss: 0.169\n",
      "[Epoch 2, Batch 600] loss: 0.175\n",
      "[Epoch 2, Batch 700] loss: 0.161\n",
      "[Epoch 2, Batch 800] loss: 0.162\n",
      "[Epoch 2, Batch 900] loss: 0.173\n",
      "[Epoch 3, Batch 100] loss: 0.133\n",
      "[Epoch 3, Batch 200] loss: 0.136\n",
      "[Epoch 3, Batch 300] loss: 0.135\n",
      "[Epoch 3, Batch 400] loss: 0.139\n",
      "[Epoch 3, Batch 500] loss: 0.126\n",
      "[Epoch 3, Batch 600] loss: 0.136\n",
      "[Epoch 3, Batch 700] loss: 0.117\n",
      "[Epoch 3, Batch 800] loss: 0.124\n",
      "[Epoch 3, Batch 900] loss: 0.122\n",
      "[Epoch 4, Batch 100] loss: 0.109\n",
      "[Epoch 4, Batch 200] loss: 0.111\n",
      "[Epoch 4, Batch 300] loss: 0.111\n",
      "[Epoch 4, Batch 400] loss: 0.118\n",
      "[Epoch 4, Batch 500] loss: 0.095\n",
      "[Epoch 4, Batch 600] loss: 0.105\n",
      "[Epoch 4, Batch 700] loss: 0.109\n",
      "[Epoch 4, Batch 800] loss: 0.107\n",
      "[Epoch 4, Batch 900] loss: 0.108\n",
      "[Epoch 5, Batch 100] loss: 0.090\n",
      "[Epoch 5, Batch 200] loss: 0.079\n",
      "[Epoch 5, Batch 300] loss: 0.092\n",
      "[Epoch 5, Batch 400] loss: 0.093\n",
      "[Epoch 5, Batch 500] loss: 0.093\n",
      "[Epoch 5, Batch 600] loss: 0.087\n",
      "[Epoch 5, Batch 700] loss: 0.087\n",
      "[Epoch 5, Batch 800] loss: 0.094\n",
      "[Epoch 5, Batch 900] loss: 0.081\n",
      "[Epoch 6, Batch 100] loss: 0.078\n",
      "[Epoch 6, Batch 200] loss: 0.071\n",
      "[Epoch 6, Batch 300] loss: 0.079\n",
      "[Epoch 6, Batch 400] loss: 0.078\n",
      "[Epoch 6, Batch 500] loss: 0.088\n",
      "[Epoch 6, Batch 600] loss: 0.085\n",
      "[Epoch 6, Batch 700] loss: 0.085\n",
      "[Epoch 6, Batch 800] loss: 0.078\n",
      "[Epoch 6, Batch 900] loss: 0.072\n",
      "[Epoch 7, Batch 100] loss: 0.060\n",
      "[Epoch 7, Batch 200] loss: 0.067\n",
      "[Epoch 7, Batch 300] loss: 0.059\n",
      "[Epoch 7, Batch 400] loss: 0.072\n",
      "[Epoch 7, Batch 500] loss: 0.063\n",
      "[Epoch 7, Batch 600] loss: 0.073\n",
      "[Epoch 7, Batch 700] loss: 0.081\n",
      "[Epoch 7, Batch 800] loss: 0.071\n",
      "[Epoch 7, Batch 900] loss: 0.078\n",
      "[Epoch 8, Batch 100] loss: 0.048\n",
      "[Epoch 8, Batch 200] loss: 0.056\n",
      "[Epoch 8, Batch 300] loss: 0.070\n",
      "[Epoch 8, Batch 400] loss: 0.067\n",
      "[Epoch 8, Batch 500] loss: 0.064\n",
      "[Epoch 8, Batch 600] loss: 0.062\n",
      "[Epoch 8, Batch 700] loss: 0.066\n",
      "[Epoch 8, Batch 800] loss: 0.055\n",
      "[Epoch 8, Batch 900] loss: 0.061\n",
      "[Epoch 9, Batch 100] loss: 0.044\n",
      "[Epoch 9, Batch 200] loss: 0.049\n",
      "[Epoch 9, Batch 300] loss: 0.054\n",
      "[Epoch 9, Batch 400] loss: 0.054\n",
      "[Epoch 9, Batch 500] loss: 0.060\n",
      "[Epoch 9, Batch 600] loss: 0.064\n",
      "[Epoch 9, Batch 700] loss: 0.061\n",
      "[Epoch 9, Batch 800] loss: 0.058\n",
      "[Epoch 9, Batch 900] loss: 0.050\n",
      "[Epoch 10, Batch 100] loss: 0.041\n",
      "[Epoch 10, Batch 200] loss: 0.053\n",
      "[Epoch 10, Batch 300] loss: 0.048\n",
      "[Epoch 10, Batch 400] loss: 0.047\n",
      "[Epoch 10, Batch 500] loss: 0.047\n",
      "[Epoch 10, Batch 600] loss: 0.047\n",
      "[Epoch 10, Batch 700] loss: 0.042\n",
      "[Epoch 10, Batch 800] loss: 0.052\n",
      "[Epoch 10, Batch 900] loss: 0.053\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = SimpleANN()\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(10):  # 10 에포크 동안 학습\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화\n",
    "        # 결과\n",
    "        outputs = model(inputs)\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # 최적화\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 출력\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 매 100 미니배치마다 출력\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nn.CrossEntropyLoss`: 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다. 예측 값과 실제 값 사이의 교차 엔트로피 손실을 계산합니다.\n",
    "- `optim.SGD`: 확률적 경사 하강법(Stochastic Gradient Descent) 최적화 알고리즘을 정의합니다.\n",
    "    - lr은 학습률, momentum은 모멘텀 값을 지정합니다.\n",
    "- `optimizer.zero_grad()`: 이전 단계에서 계산된 기울기를 초기화합니다.\n",
    "- `loss.backward()`: 역전파를 통해 기울기를 계산합니다.\n",
    "- `optimizer.step()`: 계산된 기울기를 바탕으로 가중치를 업데이트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노트\n",
    "\n",
    "lr 은 하나의 학습에서 줄이고자 하는 값인데 이 값이 너무 크면 학습이 발산할 수 있고 너무 작으면 학습이 수렴하지 않을 수 있습니다.\n",
    "\n",
    "momentum 은 모멘텀 값을 지정합니다. 모멘텀 값은 0과 1 사이의 값을 가지며, 0은 모멘텀이 없는 경우를 의미하고, 1은 모멘텀이 있는 경우를 의미합니다.\n",
    "\n",
    "loss를 남겨서 잘 진행되고 있는지 확인해주는게 좋음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 시키지 않고 결과를 볼 때는 model을 초기화해서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 7.92%\n"
     ]
    }
   ],
   "source": [
    "# model = SimpleANN()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
