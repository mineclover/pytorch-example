{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS_t0-ik_WC1"
   },
   "source": [
    "# Linear Regression 실습\n",
    "\n",
    "이번 실습에서는 linear regression에 대한 gradient descent를 직접 구현해봅니다. 여기서 사용할 문제들은 크게 두 가지로 OR 문제와 XOR 문제입니다.\n",
    "\n",
    "먼저 필요한 library들을 import합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723361278387,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "DEJFJkL6qHB9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import List, Tuple, Dict, Any, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "텐서로 정의되는 2차원이 4개 들어가있는 x 한개를 (4,2) 라고 부른다?\n",
    "아님 2차원 데이터 4개?를 표현한 것?\n",
    "\n",
    "### randn 의 정의\n",
    "\n",
    "randn 은 표준 정규 분포에서 랜덤 샘플을 추출하는 함수이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1차원 텐서 생성 (길이 4)\n",
    "x = torch.randn(4)\n",
    "# 결과 예시: tensor([-2.1436, 0.9966, 2.3426, -0.6366])\n",
    "\n",
    "# 2차원 텐서 생성 (2행 3열)\n",
    "y = torch.randn(2, 3)\n",
    "# 결과 예시: tensor([[ 1.5954, 2.8929, -1.0923],\n",
    "#                   [ 1.1719, -0.4709, -0.1996]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG2fJsOF8LsP"
   },
   "source": [
    "## OR Problem\n",
    "\n",
    "OR은 0 또는 1의 값을 가질 수 있는 두 개의 정수를 입력으로 받아 둘 중에 하나라도 1이면 1을 출력하고 아니면 0을 출력하는 문제입니다.\n",
    "즉, 우리가 학습하고자 하는 함수는 2개의 정수를 입력받아 하나의 정수를 출력하면됩니다. 이러한 함수를 학습하기 위한 data는 다음과 같이 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "텐서를 정의하는 개념에 대해 설명한다\n",
    "우선 하나의 텐서에 대한 정의로 n차원이든 정의된다\n",
    "\n",
    "그리고 그 크기에 대해 사이즈를 아래 예시의 2차원 백터 4 개는 [4,2] 처럼 정의된다\n",
    "여기서 사람의 해석 방법과 텐서의 해석 방법이 다른데\n",
    "텐서는 데이터의 개수를 먼저 정의하고 그 뒤에 데이터의 차원을 세고 있는 것을 알 수 있다\n",
    "\n",
    "```\n",
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "]) \n",
    "```\n",
    "에 대해 해석할 때\n",
    "\n",
    "해석하면 다음과 같다 일단 텐서에는 4개의 A 텐서가 있다 > A는 벡터를 담고 있는 텐서이다?\n",
    "이러한 텐서의 정의를 통해 우리는 데이터의 개수와 데이터의 차원을 명확하게 알 수 있고 이를 통해 우리는 데이터의 형태를 파악할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 x와 y는 입력 데이터와 출력 데이터다\n",
    "\n",
    "or이기 때문에 정답이 0,1,1,1 인 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1723361278864,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "SsEdD6T7qLJH",
    "outputId": "a8c91b61-98b4-45af-be7b-865abe2c8b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "y = torch.tensor([0, 1, 1, 1])\n",
    "\n",
    "print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyD1n6wf_3ey"
   },
   "source": [
    "출력 결과에서 볼 수 있다시피 $x$의 shape은 (4, 2)로, 총 4개의 two-dimensional data 임을 알 수 있습니다. $y$는 각 $x_i$에 대한 label로 우리가 설정한 문제의 조건을 잘 따라가는 것을 알 수 있습니다.\n",
    "\n",
    "다음으로는 linear regression의 parameter들인 $w, b$를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "uzG4w1VYqlhz",
    "outputId": "8c5ad5c5-dea3-4b59-cebe-feeca50507f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([1, 1])\n",
      "tensor([[0.3566, 0.1005]]) tensor([[0.1993]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn((1, 2))\n",
    "b = torch.randn((1, 1))\n",
    "\n",
    "print(w.shape, b.shape)\n",
    "print(w, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELTb9Dl-AYbp"
   },
   "source": [
    "$w$는 1x2의 벡터이고 $b$는 1x1의 scalar임을 알 수 있습니다. 여기서는 `torch.randn`을 사용하여 standard normal distribution을 가지고 초기화하였습니다.\n",
    "\n",
    "이러한 $w, b$와 data $x, y$가 주어졌을 때 우리가 학습한 $w, b$의 성능을 평가하는 함수를 구현합시다.\n",
    "평가 함수는 다음과 같이 MSE로 정의됩니다:\n",
    "$$l(f) := MSE(f(x), y) = \\frac{1}{n} \\sum_{i=1}^n (f(x_i) - y)^2.$$\n",
    "이를 구현한 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "여기서 w는 전체 영역의 크기에 대해 정의하고 있고\n",
    "b는 한개의 스칼라를 정의하고 있음을 알 수 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred 는 결과 출력할 때 사용한 함수임\n",
    "\n",
    "w는 전체 영역의 현재 가중치\n",
    "x.T 는 현재 \n",
    "b는 현재 입력 값에 대한 편향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "LBxldV7D8UMf"
   },
   "outputs": [],
   "source": [
    "def pred(w, b, x):\n",
    "  print('pred:',w, b, x ,x.T)\n",
    "  return torch.matmul(w, x.T) + b\n",
    "\n",
    "\n",
    "def loss(w, b, x, y):\n",
    "  return (y - pred(w, b, x)).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmM79Ly6VyBw"
   },
   "source": [
    "먼저 `def pred(w, b, x)`는 $wx^T + b$, 즉 1차 함수 $f$의 $x$에 대한 결과를 반환하는 함수를 구현했습니다.\n",
    "이를 이용하여 주어진 label $y$와의 MSE를 측정하는 코드가 `def loss(w, b, x, y)`에 구현되어있습니다.\n",
    "\n",
    "다음은 MSE를 기반으로 $w, b$의 gradient를 구하는 코드를 구현하겠습니다.\n",
    "MSE에 대한 $w$의 gradient는 다음과 같이 구할 수 있습니다:\n",
    "$$\\frac{\\partial l}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y)x_i.$$\n",
    "$b$에 대한 gradient는 다음과 같습니다:\n",
    "$$\\frac{\\partial l}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y).$$\n",
    "이를 코드로 구현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "rLrsXZ0iq13m"
   },
   "outputs": [],
   "source": [
    "def grad_w(w, b, x, y):\n",
    "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
    "  tmp1 = torch.matmul(w, x.T)  # (1, n)\n",
    "  tmp2 = tmp1 + b              # (1, n)\n",
    "  tmp3 = 2 * (tmp2 - y[None])  # (1, n)\n",
    "  grad_item = tmp3.T * x       # (n, d)\n",
    "  return grad_item.mean(dim=0, keepdim=True)  # (1, d)\n",
    "\n",
    "\n",
    "def grad_b(w, b, x, y):\n",
    "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
    "  grad_item = 2 * (torch.matmul(w, x.T) + b - y[None])  # (1, n)\n",
    "  return grad_item.mean(dim=-1, keepdim=True)           # (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCbBU1RaX6O5"
   },
   "source": [
    "여기서 중요한 것은 shape에 맞춰서 연산을 잘 사용해야한다는 것입니다. Shape과 관련된 설명은 `[Chapter 0]`의 Numpy에서 설명했으니, 복습하신다는 느낌으로 주석으로 써놓은 shape들을 유도해보시면 좋을 것 같습니다. 중요한 것은 반환되는 tensor의 shape이 우리가 구하고자 하는 gradient와 일치해야 한다는 것입니다. 예를 들어 $w$의 $l$에 대한 gradient는 $w$와 shape이 동일해야 합니다.\n",
    "\n",
    "마지막으로 gradient descent 함수를 구현하겠습니다. Gradient descent는 다음과 같이 정의됩니다:\n",
    "$$w^{(new)} = w^{(old)} - \\eta \\frac{\\partial l}{\\partial w} \\biggr\\rvert_{w = w^{(old)}}.$$\n",
    "Gradient는 위에서 구현했으니 이를 활용하여 learning rate $\\eta$가 주어졌을 때 $w, b$를 update하는 코드를 구현할 수 있습니다. 구현한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "wFRS72UF8QVv"
   },
   "outputs": [],
   "source": [
    "def update(x, y, w, b, lr):\n",
    "  w = w - lr * grad_w(w, b, x, y)\n",
    "  b = b - lr * grad_b(w, b, x, y)\n",
    "  return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b93uvneVZ7bF"
   },
   "source": [
    "Gradient descent에 해당하는 코드는 모두 구현하였습니다. 이제 학습하는 코드를 구현하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Pa6fA_ZUFI-0"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, lr, w, b, x, y):\n",
    "  for e in range(n_epochs):\n",
    "    w, b = update(x, y, w, b, lr)\n",
    "    print(f\"Epoch {e:3d} | Loss: {loss(w, b, x, y)}\")\n",
    "  return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrJGKWilaBFq"
   },
   "source": [
    "여기서 `n_epochs`는 update를 하는 횟수를 의미합니다. 매 update 이후에 `loss` 함수를 사용하여 잘 수렴하고 있는지 살펴봅니다. 실제로 이 함수를 실행한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "zFk-josgBSj7",
    "outputId": "ec127ca9-a563-43ac-8adc-8e1e398ecb5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 0.0624999962747097\n",
      "Epoch   1 | Loss: 0.0624999962747097\n",
      "Epoch   2 | Loss: 0.0624999962747097\n",
      "Epoch   3 | Loss: 0.0624999962747097\n",
      "Epoch   4 | Loss: 0.0624999962747097\n",
      "Epoch   5 | Loss: 0.0624999962747097\n",
      "Epoch   6 | Loss: 0.0624999962747097\n",
      "Epoch   7 | Loss: 0.0624999962747097\n",
      "Epoch   8 | Loss: 0.0624999962747097\n",
      "Epoch   9 | Loss: 0.0624999962747097\n",
      "Epoch  10 | Loss: 0.0624999962747097\n",
      "Epoch  11 | Loss: 0.0624999962747097\n",
      "Epoch  12 | Loss: 0.0624999962747097\n",
      "Epoch  13 | Loss: 0.0624999962747097\n",
      "Epoch  14 | Loss: 0.0624999962747097\n",
      "Epoch  15 | Loss: 0.0624999962747097\n",
      "Epoch  16 | Loss: 0.0624999962747097\n",
      "Epoch  17 | Loss: 0.0624999962747097\n",
      "Epoch  18 | Loss: 0.0624999962747097\n",
      "Epoch  19 | Loss: 0.0624999962747097\n",
      "Epoch  20 | Loss: 0.0624999962747097\n",
      "Epoch  21 | Loss: 0.0624999962747097\n",
      "Epoch  22 | Loss: 0.0624999962747097\n",
      "Epoch  23 | Loss: 0.0624999962747097\n",
      "Epoch  24 | Loss: 0.0624999962747097\n",
      "Epoch  25 | Loss: 0.0624999962747097\n",
      "Epoch  26 | Loss: 0.0624999962747097\n",
      "Epoch  27 | Loss: 0.0624999962747097\n",
      "Epoch  28 | Loss: 0.0624999962747097\n",
      "Epoch  29 | Loss: 0.0624999962747097\n",
      "Epoch  30 | Loss: 0.0624999962747097\n",
      "Epoch  31 | Loss: 0.0624999962747097\n",
      "Epoch  32 | Loss: 0.0624999962747097\n",
      "Epoch  33 | Loss: 0.0624999962747097\n",
      "Epoch  34 | Loss: 0.0624999962747097\n",
      "Epoch  35 | Loss: 0.0624999962747097\n",
      "Epoch  36 | Loss: 0.0624999962747097\n",
      "Epoch  37 | Loss: 0.0624999962747097\n",
      "Epoch  38 | Loss: 0.0624999962747097\n",
      "Epoch  39 | Loss: 0.0624999962747097\n",
      "Epoch  40 | Loss: 0.0624999962747097\n",
      "Epoch  41 | Loss: 0.0624999962747097\n",
      "Epoch  42 | Loss: 0.0624999962747097\n",
      "Epoch  43 | Loss: 0.0624999962747097\n",
      "Epoch  44 | Loss: 0.0624999962747097\n",
      "Epoch  45 | Loss: 0.0624999962747097\n",
      "Epoch  46 | Loss: 0.0624999962747097\n",
      "Epoch  47 | Loss: 0.0624999962747097\n",
      "Epoch  48 | Loss: 0.0624999962747097\n",
      "Epoch  49 | Loss: 0.0624999962747097\n",
      "Epoch  50 | Loss: 0.0624999962747097\n",
      "Epoch  51 | Loss: 0.0624999962747097\n",
      "Epoch  52 | Loss: 0.0624999962747097\n",
      "Epoch  53 | Loss: 0.0624999962747097\n",
      "Epoch  54 | Loss: 0.0624999962747097\n",
      "Epoch  55 | Loss: 0.0624999962747097\n",
      "Epoch  56 | Loss: 0.0624999962747097\n",
      "Epoch  57 | Loss: 0.0624999962747097\n",
      "Epoch  58 | Loss: 0.0624999962747097\n",
      "Epoch  59 | Loss: 0.0624999962747097\n",
      "Epoch  60 | Loss: 0.0624999962747097\n",
      "Epoch  61 | Loss: 0.0624999962747097\n",
      "Epoch  62 | Loss: 0.0624999962747097\n",
      "Epoch  63 | Loss: 0.0624999962747097\n",
      "Epoch  64 | Loss: 0.0624999962747097\n",
      "Epoch  65 | Loss: 0.0624999962747097\n",
      "Epoch  66 | Loss: 0.0624999962747097\n",
      "Epoch  67 | Loss: 0.0624999962747097\n",
      "Epoch  68 | Loss: 0.0624999962747097\n",
      "Epoch  69 | Loss: 0.0624999962747097\n",
      "Epoch  70 | Loss: 0.0624999962747097\n",
      "Epoch  71 | Loss: 0.0624999962747097\n",
      "Epoch  72 | Loss: 0.0624999962747097\n",
      "Epoch  73 | Loss: 0.0624999962747097\n",
      "Epoch  74 | Loss: 0.0624999962747097\n",
      "Epoch  75 | Loss: 0.0624999962747097\n",
      "Epoch  76 | Loss: 0.0624999962747097\n",
      "Epoch  77 | Loss: 0.0624999962747097\n",
      "Epoch  78 | Loss: 0.0624999962747097\n",
      "Epoch  79 | Loss: 0.0624999962747097\n",
      "Epoch  80 | Loss: 0.0624999962747097\n",
      "Epoch  81 | Loss: 0.0624999962747097\n",
      "Epoch  82 | Loss: 0.0624999962747097\n",
      "Epoch  83 | Loss: 0.0624999962747097\n",
      "Epoch  84 | Loss: 0.0624999962747097\n",
      "Epoch  85 | Loss: 0.0624999962747097\n",
      "Epoch  86 | Loss: 0.0624999962747097\n",
      "Epoch  87 | Loss: 0.0624999962747097\n",
      "Epoch  88 | Loss: 0.0624999962747097\n",
      "Epoch  89 | Loss: 0.0624999962747097\n",
      "Epoch  90 | Loss: 0.0624999962747097\n",
      "Epoch  91 | Loss: 0.0624999962747097\n",
      "Epoch  92 | Loss: 0.0624999962747097\n",
      "Epoch  93 | Loss: 0.0624999962747097\n",
      "Epoch  94 | Loss: 0.0624999962747097\n",
      "Epoch  95 | Loss: 0.0624999962747097\n",
      "Epoch  96 | Loss: 0.0624999962747097\n",
      "Epoch  97 | Loss: 0.0624999962747097\n",
      "Epoch  98 | Loss: 0.0624999962747097\n",
      "Epoch  99 | Loss: 0.0624999962747097\n",
      "tensor([[0.5000, 0.5000]]) tensor([[0.2500]])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "w, b = train(n_epochs, lr, w, b, x, y)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Ny-YkAaNh8"
   },
   "source": [
    "잘 수렴하는 것을 확인하였습니다. 마지막으로 OR data에 대한 $w, b$의 예측 결과와 label을 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "IggGP969Bh-w",
    "outputId": "c9a5ccb0-5fdc-4f86-ed30-157f56f97d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[0.3566, 0.1005]]) tensor([[0.1993]]) tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]]) tensor([[0., 0., 1., 1.],\n",
      "        [0., 1., 0., 1.]])\n",
      "tensor([[0.1993, 0.2998, 0.5559, 0.6564]])\n",
      "tensor([0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(pred(w, b, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8gKvx2naWDP"
   },
   "source": [
    "아래 값으로 수렴하는 것을 볼 수 있음\n",
    "\n",
    "tensor([[0.2500, 0.7500, 0.7500, 1.2500]])\n",
    "tensor([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 결과를 볼 수 있다시피 우리의 linear regression model은 0과 1에 해당하는 data를 잘 구분하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMXZLfd3DC50"
   },
   "source": [
    "# XOR Problem\n",
    "\n",
    "이번에는 XOR를 학습해보겠습니다. XOR은 OR과 똑같은 입력을 받는 문제로, 두 개의 0 또는 1의 정수가 들어왔을 때 두 정수가 다르면 1, 아니면 0을 출력해야 합니다.\n",
    "먼저 data를 만들어보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "JtFGsqNXCjtM",
    "outputId": "7183941e-8298-4f2d-f443-9c20f39aec11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([\n",
    "#     [0., 0.],\n",
    "#     [0., 1.],\n",
    "#     [1., 0.],\n",
    "#     [1., 1.]\n",
    "# ])\n",
    "# y = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYRtKaviaedO"
   },
   "source": [
    "보시다시피 shape이나 생성 과정은 OR과 똑같습니다. 다른 것은 $y$에서의 labeling입니다. OR과 다르게 $x = (1, 1)$에 대해서는 0을 labeling했습니다.\n",
    "이러한 사소한 차이에 대해서도 linear regression model이 잘 학습할 수 있을지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "여러번 돌려보면서 w,b 가  수렴하는 것을 볼 수 있음\n",
    "\n",
    "그런데 그것과 별개로 loss가 0.25로 수렴하는 것도 확인 할 수 있는데\n",
    "왜 그렇게 되는지 의문임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "iw5UUqKdDG98",
    "outputId": "b37eb7e7-ff7f-4887-e432-c5f597179f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 0.06256163865327835\n",
      "Epoch   1 | Loss: 0.06255756318569183\n",
      "Epoch   2 | Loss: 0.06255373358726501\n",
      "Epoch   3 | Loss: 0.06255017220973969\n",
      "Epoch   4 | Loss: 0.06254684925079346\n",
      "Epoch   5 | Loss: 0.06254371255636215\n",
      "Epoch   6 | Loss: 0.06254082173109055\n",
      "Epoch   7 | Loss: 0.06253809481859207\n",
      "Epoch   8 | Loss: 0.06253557652235031\n",
      "Epoch   9 | Loss: 0.06253322213888168\n",
      "Epoch  10 | Loss: 0.0625310093164444\n",
      "Epoch  11 | Loss: 0.06252896040678024\n",
      "Epoch  12 | Loss: 0.06252701580524445\n",
      "Epoch  13 | Loss: 0.06252523511648178\n",
      "Epoch  14 | Loss: 0.06252355873584747\n",
      "Epoch  15 | Loss: 0.06252199411392212\n",
      "Epoch  16 | Loss: 0.06252053380012512\n",
      "Epoch  17 | Loss: 0.06251917779445648\n",
      "Epoch  18 | Loss: 0.06251788884401321\n",
      "Epoch  19 | Loss: 0.0625167042016983\n",
      "Epoch  20 | Loss: 0.06251560896635056\n",
      "Epoch  21 | Loss: 0.06251457333564758\n",
      "Epoch  22 | Loss: 0.06251358985900879\n",
      "Epoch  23 | Loss: 0.06251268833875656\n",
      "Epoch  24 | Loss: 0.0625118687748909\n",
      "Epoch  25 | Loss: 0.06251107156276703\n",
      "Epoch  26 | Loss: 0.06251032650470734\n",
      "Epoch  27 | Loss: 0.06250963360071182\n",
      "Epoch  28 | Loss: 0.06250900030136108\n",
      "Epoch  29 | Loss: 0.06250841915607452\n",
      "Epoch  30 | Loss: 0.06250784546136856\n",
      "Epoch  31 | Loss: 0.06250731647014618\n",
      "Epoch  32 | Loss: 0.06250683963298798\n",
      "Epoch  33 | Loss: 0.06250637769699097\n",
      "Epoch  34 | Loss: 0.06250597536563873\n",
      "Epoch  35 | Loss: 0.0625055581331253\n",
      "Epoch  36 | Loss: 0.06250518560409546\n",
      "Epoch  37 | Loss: 0.062504842877388\n",
      "Epoch  38 | Loss: 0.06250452250242233\n",
      "Epoch  39 | Loss: 0.06250423192977905\n",
      "Epoch  40 | Loss: 0.06250394880771637\n",
      "Epoch  41 | Loss: 0.06250369548797607\n",
      "Epoch  42 | Loss: 0.06250343471765518\n",
      "Epoch  43 | Loss: 0.06250321865081787\n",
      "Epoch  44 | Loss: 0.06250298023223877\n",
      "Epoch  45 | Loss: 0.06250280141830444\n",
      "Epoch  46 | Loss: 0.06250260770320892\n",
      "Epoch  47 | Loss: 0.06250245124101639\n",
      "Epoch  48 | Loss: 0.06250227987766266\n",
      "Epoch  49 | Loss: 0.06250213086605072\n",
      "Epoch  50 | Loss: 0.06250198185443878\n",
      "Epoch  51 | Loss: 0.06250184029340744\n",
      "Epoch  52 | Loss: 0.06250174343585968\n",
      "Epoch  53 | Loss: 0.06250160932540894\n",
      "Epoch  54 | Loss: 0.06250150501728058\n",
      "Epoch  55 | Loss: 0.06250141561031342\n",
      "Epoch  56 | Loss: 0.06250132620334625\n",
      "Epoch  57 | Loss: 0.06250123679637909\n",
      "Epoch  58 | Loss: 0.06250114738941193\n",
      "Epoch  59 | Loss: 0.06250105798244476\n",
      "Epoch  60 | Loss: 0.06250099837779999\n",
      "Epoch  61 | Loss: 0.06250092387199402\n",
      "Epoch  62 | Loss: 0.06250087916851044\n",
      "Epoch  63 | Loss: 0.06250080466270447\n",
      "Epoch  64 | Loss: 0.06250075995922089\n",
      "Epoch  65 | Loss: 0.06250070035457611\n",
      "Epoch  66 | Loss: 0.06250065565109253\n",
      "Epoch  67 | Loss: 0.06250060349702835\n",
      "Epoch  68 | Loss: 0.06250058114528656\n",
      "Epoch  69 | Loss: 0.06250055134296417\n",
      "Epoch  70 | Loss: 0.06250052154064178\n",
      "Epoch  71 | Loss: 0.0625004693865776\n",
      "Epoch  72 | Loss: 0.06250043213367462\n",
      "Epoch  73 | Loss: 0.06250042468309402\n",
      "Epoch  74 | Loss: 0.06250037997961044\n",
      "Epoch  75 | Loss: 0.06250035762786865\n",
      "Epoch  76 | Loss: 0.06250034272670746\n",
      "Epoch  77 | Loss: 0.06250031292438507\n",
      "Epoch  78 | Loss: 0.06250029057264328\n",
      "Epoch  79 | Loss: 0.06250026822090149\n",
      "Epoch  80 | Loss: 0.06250026077032089\n",
      "Epoch  81 | Loss: 0.0625002384185791\n",
      "Epoch  82 | Loss: 0.06250022351741791\n",
      "Epoch  83 | Loss: 0.06250021606683731\n",
      "Epoch  84 | Loss: 0.06250017881393433\n",
      "Epoch  85 | Loss: 0.06250017136335373\n",
      "Epoch  86 | Loss: 0.06250017881393433\n",
      "Epoch  87 | Loss: 0.06250015646219254\n",
      "Epoch  88 | Loss: 0.06250014901161194\n",
      "Epoch  89 | Loss: 0.06250013411045074\n",
      "Epoch  90 | Loss: 0.06250013411045074\n",
      "Epoch  91 | Loss: 0.06250012665987015\n",
      "Epoch  92 | Loss: 0.06250010430812836\n",
      "Epoch  93 | Loss: 0.06250010430812836\n",
      "Epoch  94 | Loss: 0.06250010430812836\n",
      "Epoch  95 | Loss: 0.06250008195638657\n",
      "Epoch  96 | Loss: 0.06250009685754776\n",
      "Epoch  97 | Loss: 0.06250007450580597\n",
      "Epoch  98 | Loss: 0.06250008195638657\n",
      "Epoch  99 | Loss: 0.06250005960464478\n",
      "tensor([[0.5004, 0.5004]]) tensor([[0.2496]])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "w, b = train(n_epochs, lr, w, b, x, y)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8sMLaJ9a770"
   },
   "source": [
    "이전과는 다르게 loss가 1.0보다 작아지지 않는 것을 알 수 있습니다. 실제 예측 결과를 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "L81iXxgHDIq2",
    "outputId": "1fc0cef5-f24d-45e7-bc0a-fc91c82762b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2496, 0.7499, 0.7499, 1.2503]])\n",
      "tensor([0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(pred(w, b, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuqkwJ2NbB7S"
   },
   "source": [
    "보시다시피 0과 1에 해당하는 data들을 잘 구분하지 못하는 모습니다. Linear regression model은 XOR을 잘 처리하지 못하는 것을 우리는 이번 실습을 통해 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "2zAy7YgFDMgx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2/ufUFmLdn0BAFVwEnpdC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
