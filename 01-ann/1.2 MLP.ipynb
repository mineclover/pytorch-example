{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkat-RaWc1u4"
   },
   "source": [
    "# MLP 실습\n",
    "\n",
    "이번 실습에서는 PyTorch에서 MLP 및 backpropagation을 구현하여 XOR 문제에 학습해봅니다. 먼저 필요한 library들을 import합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8556,
     "status": "ok",
     "timestamp": 1723364650516,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "DEJFJkL6qHB9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC0DqhmtdBuK"
   },
   "source": [
    "그 다음 공유된 notebook과 똑같은 결과를 얻기 위한 코드를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "nnXpVhJXbRzw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = 7777\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmAh1sEJdKUN"
   },
   "source": [
    "위의 코드에서는 seed 고정과 `cudnn.deterministic`, `cudnn.benchmark`를 조정하였습니다. 자세한 사항들은 https://pytorch.org/docs/stable/notes/randomness.html에서 찾아볼 수 있습니다.\n",
    "\n",
    "다음은 이전 실습과 같은 코드로 XOR data를 생성하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "SsEdD6T7qLJH",
    "outputId": "46a82aae-b239-4f92-b1f8-2745101a69bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "y = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sSAu0HKdlRc"
   },
   "source": [
    "Data를 만들었으니 이제 MLP를 구현해야 합니다. 여기서 알아둬야할 점들은 다음과 같습니다:\n",
    "- PyTorch에서는 우리가 학습하고자 하는 함수 $f$를 보통 `torch.nn.Module` class를 상속받은 class로 구현합니다.\n",
    "- `torch.nn.Module` class는 abstract class로, `def forward`를 구현하도록 abstractmethod를 제공합니다. 이 method는 $f(x)$, 즉 함수의 출력에 해당됩니다.\n",
    "- PyTorch에서는 선형함수를 `torch.nn.Linear` class로 구현할 수 있습니다.\n",
    "- 마찬가지로 ReLU도 `torch.nn.ReLU`로 제공하고 있습니다.\n",
    "\n",
    "위의 점들을 활용하여 2-layer MLP를 구현한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "KZe7xFjMqfc0"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, d, d_prime):\n",
    "    super().__init__()\n",
    "\n",
    "    self.layer1 = nn.Linear(d, d_prime)\n",
    "    self.layer2 = nn.Linear(d_prime, 1)\n",
    "    self.act = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: (n, d)\n",
    "    x = self.layer1(x)  # (n, d_prime)\n",
    "    x = self.act(x)     # (n, d_prime)\n",
    "    x = self.layer2(x)  # (n, 1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = Model(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjPzBGayeZ2f"
   },
   "source": [
    "우리가 원하는 것은 $x \\in \\mathbb{R}^{n \\times d}$가 입력으로 들어왔을 때 첫 번째 선형 함수를 거쳤을 때의 dimension이 $\\mathbb{R}^{n \\times d'}$이 되면서 두 번째 선형 함수를 거쳤을 때는 $\\mathbb{R}^{n \\times 1}$이 되기를 원합니다.\n",
    "첫 번째 선형함수는 `self.layer1 = nn.Linear(d, d_prime)`로 구현할 수 있으며, `nn.Linear`의 첫 번째 인자는 입력으로 들어오는 tensor의 마지막 dimension, 두 번째 인자는 출력되는 tensor의 마지막 dimension을 뜻합니다.\n",
    "이 정보를 이용하면 두 번째 선형함수도 `self.layer2 = nn.Linear(d_prime, 1)`로 구현할 수 있습니다.\n",
    "마찬가지로 ReLU도 `nn.ReLU()`로 선언할 수 있습니다.\n",
    "\n",
    "이제 $f(x)$에 대한 구현은 `def forward(self, x)`에서 할 수 있습니다.\n",
    "생성자에서 선언한 세 개의 layer들을 순차적으로 지나면 우리가 원하던 결과를 얻을 수 있습니다.\n",
    "여기서도 shape의 변화를 돌려보지 않고 예측하는 것이 실제로 구현하고 디버깅할 때 중요하게 여겨집니다.\n",
    "\n",
    "마지막 줄에서는 입력 dimension이 2이고, 중간 dimension이 10이 되는 2-layer MLP에 대한 object를 생성하였습니다.\n",
    "다음은 PyTorch에서 gradient descent를 어떻게 구현하는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4678,
     "status": "ok",
     "timestamp": 1723364655186,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "NpygQ87grSJV"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HraI34csgTYL"
   },
   "source": [
    "PyTorch는 다양한 update 알고리즘들을 `torch.optim`에서 제공하고 있습니다.\n",
    "우리는 gradient descent를 사용할 것이기 때문에 `SGD` class를 불러옵니다.\n",
    "`SGD`는 첫 번째 인자로 업데이트를 할 parameter들의 list를 받습니다. 예를 들어 선형 함수에서의 $w, b$가 있습니다.\n",
    "PyTorch의 `nn.Module` class는 이러한 것들을 잘 정의해주기 때문에 `model.parameters()`의 형식으로 넘겨주기만 하면 됩니다.\n",
    "두 번째 인자는 learning rate로, 이전의 gradient descent에서 사용하던 learning rate와 똑같은 역할을 가지고 있습니다.\n",
    "\n",
    "다음은 실제 학습 코드를 구현하며, backpropagation이 어떻게 이루어지는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723364655186,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Pa6fA_ZUFI-0"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, model, optimizer, x, y):\n",
    "  for e in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred[:, 0] - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {e:3d} | Loss: {loss}\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heaHQYZVhBAt"
   },
   "source": [
    "`model(x)`를 통해 우리가 구현한 `forward` 함수를 사용할 수 있습니다. 그리고 이를 이용하여 MSE loss로 `model`을 평가할 수 있습니다.\n",
    "Gradient descent의 구현은 다음 세 가지 부분에서 진행되고 있습니다:\n",
    "- 본격적으로 학습이 진행되기 전에 `model.zero_grad()`를 실행합니다. 이는 각 parameter의 gradient 값이 저장되어 있을 수도 있기 때문에 지워주는 기능을 수행합니다.\n",
    "- loss를 계산한 후, `loss.backward()`를 진행합니다. backward 함수를 실행하면 `model`에 있는 모든 parameter들의 `loss`에 대한 gradient를 계산하게 됩니다.\n",
    "- 마지막으로 계산한 gradient들을 가지고 parameter들을 update하는 것은 `optimizer.step()`을 통해 진행하게 됩니다. optimizer는 이전에 인자로 받았던 `model.parameters()`에 해당하는 parameter들만 update를 진행하게 됩니다.\n",
    "\n",
    "위의 코드로 학습을 진행한 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "zFk-josgBSj7",
    "outputId": "10343a67-4055-4b7d-e103-34a1d043d01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 2.8124639987945557\n",
      "Epoch   1 | Loss: 2.309717893600464\n",
      "Epoch   2 | Loss: 1.6600805521011353\n",
      "Epoch   3 | Loss: 0.9176857471466064\n",
      "Epoch   4 | Loss: 0.8310231566429138\n",
      "Epoch   5 | Loss: 0.7857464551925659\n",
      "Epoch   6 | Loss: 0.7469707727432251\n",
      "Epoch   7 | Loss: 0.7105731964111328\n",
      "Epoch   8 | Loss: 0.6797015070915222\n",
      "Epoch   9 | Loss: 0.6505993604660034\n",
      "Epoch  10 | Loss: 0.6153719425201416\n",
      "Epoch  11 | Loss: 0.5817742347717285\n",
      "Epoch  12 | Loss: 0.5496706962585449\n",
      "Epoch  13 | Loss: 0.5309391617774963\n",
      "Epoch  14 | Loss: 0.500799834728241\n",
      "Epoch  15 | Loss: 0.4737371504306793\n",
      "Epoch  16 | Loss: 0.45748329162597656\n",
      "Epoch  17 | Loss: 0.43399757146835327\n",
      "Epoch  18 | Loss: 0.4096483588218689\n",
      "Epoch  19 | Loss: 0.38796401023864746\n",
      "Epoch  20 | Loss: 0.3663795292377472\n",
      "Epoch  21 | Loss: 0.348500519990921\n",
      "Epoch  22 | Loss: 0.3266391158103943\n",
      "Epoch  23 | Loss: 0.3139018416404724\n",
      "Epoch  24 | Loss: 0.28671738505363464\n",
      "Epoch  25 | Loss: 0.2610698342323303\n",
      "Epoch  26 | Loss: 0.23623433709144592\n",
      "Epoch  27 | Loss: 0.21415171027183533\n",
      "Epoch  28 | Loss: 0.206107959151268\n",
      "Epoch  29 | Loss: 0.1822202056646347\n",
      "Epoch  30 | Loss: 0.16103476285934448\n",
      "Epoch  31 | Loss: 0.1481526792049408\n",
      "Epoch  32 | Loss: 0.1299654245376587\n",
      "Epoch  33 | Loss: 0.12578867375850677\n",
      "Epoch  34 | Loss: 0.1144099161028862\n",
      "Epoch  35 | Loss: 0.1180308610200882\n",
      "Epoch  36 | Loss: 0.11135037243366241\n",
      "Epoch  37 | Loss: 0.09905606508255005\n",
      "Epoch  38 | Loss: 0.10490961372852325\n",
      "Epoch  39 | Loss: 0.08657155930995941\n",
      "Epoch  40 | Loss: 0.08665241301059723\n",
      "Epoch  41 | Loss: 0.0664956197142601\n",
      "Epoch  42 | Loss: 0.07306360453367233\n",
      "Epoch  43 | Loss: 0.06029706448316574\n",
      "Epoch  44 | Loss: 0.058103740215301514\n",
      "Epoch  45 | Loss: 0.06295835226774216\n",
      "Epoch  46 | Loss: 0.06997162103652954\n",
      "Epoch  47 | Loss: 0.056175798177719116\n",
      "Epoch  48 | Loss: 0.06725446879863739\n",
      "Epoch  49 | Loss: 0.05403393507003784\n",
      "Epoch  50 | Loss: 0.07560430467128754\n",
      "Epoch  51 | Loss: 0.05889689177274704\n",
      "Epoch  52 | Loss: 0.05406159535050392\n",
      "Epoch  53 | Loss: 0.0462351068854332\n",
      "Epoch  54 | Loss: 0.0598265640437603\n",
      "Epoch  55 | Loss: 0.04889870434999466\n",
      "Epoch  56 | Loss: 0.05164020508527756\n",
      "Epoch  57 | Loss: 0.04841768369078636\n",
      "Epoch  58 | Loss: 0.06241666153073311\n",
      "Epoch  59 | Loss: 0.05121435597538948\n",
      "Epoch  60 | Loss: 0.05042683705687523\n",
      "Epoch  61 | Loss: 0.05139951780438423\n",
      "Epoch  62 | Loss: 0.06424298882484436\n",
      "Epoch  63 | Loss: 0.052223458886146545\n",
      "Epoch  64 | Loss: 0.05135074257850647\n",
      "Epoch  65 | Loss: 0.04424873739480972\n",
      "Epoch  66 | Loss: 0.05591907352209091\n",
      "Epoch  67 | Loss: 0.046582601964473724\n",
      "Epoch  68 | Loss: 0.04642226919531822\n",
      "Epoch  69 | Loss: 0.040153712034225464\n",
      "Epoch  70 | Loss: 0.05006988346576691\n",
      "Epoch  71 | Loss: 0.04217236116528511\n",
      "Epoch  72 | Loss: 0.042123790830373764\n",
      "Epoch  73 | Loss: 0.03611898049712181\n",
      "Epoch  74 | Loss: 0.04397241398692131\n",
      "Epoch  75 | Loss: 0.03726819157600403\n",
      "Epoch  76 | Loss: 0.037098478525877\n",
      "Epoch  77 | Loss: 0.03176146373152733\n",
      "Epoch  78 | Loss: 0.031489163637161255\n",
      "Epoch  79 | Loss: 0.02708084136247635\n",
      "Epoch  80 | Loss: 0.026670511811971664\n",
      "Epoch  81 | Loss: 0.023186251521110535\n",
      "Epoch  82 | Loss: 0.02739042416214943\n",
      "Epoch  83 | Loss: 0.023494206368923187\n",
      "Epoch  84 | Loss: 0.022942181676626205\n",
      "Epoch  85 | Loss: 0.019843928515911102\n",
      "Epoch  86 | Loss: 0.01930542290210724\n",
      "Epoch  87 | Loss: 0.016771741211414337\n",
      "Epoch  88 | Loss: 0.016231024637818336\n",
      "Epoch  89 | Loss: 0.014146436005830765\n",
      "Epoch  90 | Loss: 0.013615904375910759\n",
      "Epoch  91 | Loss: 0.01190036628395319\n",
      "Epoch  92 | Loss: 0.011394335888326168\n",
      "Epoch  93 | Loss: 0.009984711185097694\n",
      "Epoch  94 | Loss: 0.009513604454696178\n",
      "Epoch  95 | Loss: 0.00835750624537468\n",
      "Epoch  96 | Loss: 0.007927638478577137\n",
      "Epoch  97 | Loss: 0.00698112603276968\n",
      "Epoch  98 | Loss: 0.006595130078494549\n",
      "Epoch  99 | Loss: 0.005821185186505318\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "model = train(n_epochs, model, optimizer, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCMcj41Ghu7w"
   },
   "source": [
    "Linear regression과는 달리 잘 수렴하는 모습을 보여주고 있습니다. 특히, 우리가 직접 gradient나 gradient descent를 구현하지 않아도 주어진 data를 잘 학습하는 코드를 PyTorch를 통해 구현할 수 있었습니다. 마지막으로 예측 결과를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "IggGP969Bh-w",
    "outputId": "d555310d-7c24-4f40-c80b-571f44a217fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0208],\n",
      "        [1.0484],\n",
      "        [1.0156],\n",
      "        [0.0496]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(model(x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuAOyBfKiTJL"
   },
   "source": [
    "매우 정확한 예측 결과를 보여주고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "2zAy7YgFDMgx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0Gd7VoUSDzxEwJov/s6Wg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
